# FeelWave

### ğŸ“¥ Code Download

The FeelWave code package can be downloaded here: https://pan.ustc.edu.cn/share/index/8e1ee29b7d6e4a78ae25.

### ğŸ¬ Demo Video

A demonstration of FeelWave is available at:
**YouTube Demo:** [https://youtu.be/j-PLP6Tuc9g](https://youtu.be/j-PLP6Tuc9g).

This repository contains the core implementation of the **cross-modal emotion distillation framework**, where an audio-based teacher model transfers emotion-relevant representations to a mmWave-based student model.

---

## ğŸ“¦ Environment

The experiments were conducted under the following environment:

* Python 3.10.16
* TensorFlow 2.15.0
* Keras 2.15.0 (standalone Keras; `tf.keras` corresponds to TensorFlow 2.15.0)

---

## ğŸ“ Repository Structure

```
FeelWave_Code
â”œâ”€â”€ Backbone/                     # Model architectures
â”‚   â”œâ”€â”€ autopool.py               # AutoPool operator
â”‚   â”œâ”€â”€ mobilenetv3_variant.py    # MobileNetV3-Small variant (mmWave student)
â”‚   â”œâ”€â”€ models.py                 # Audio teacher model (TRILL + word timing)
â”‚   â””â”€â”€ saved_model_trill/        # Pretrained TRILL checkpoint
â”‚       â”œâ”€â”€ saved_model.pb
â”‚       â””â”€â”€ variables/
â”‚           â”œâ”€â”€ variables.data-00000-of-00001
â”‚           â””â”€â”€ variables.index
â”‚
â”œâ”€â”€ cross_model_transfer.py       # Distillation pipeline (WCL + CE)
â”‚
â”œâ”€â”€ result/                       # Training outputs
â”‚   â”œâ”€â”€ saved_teacher_model/
â”‚   â”œâ”€â”€ saved_student_model/
â”‚   â”œâ”€â”€ fold.json
â”‚   â”œâ”€â”€ teacher_fold.txt
â”‚   â””â”€â”€ distill_fold.txt
```

---

# EmoDataset: Paired mmWaveâ€“Audio Emotional Speech Dataset

## ğŸ“˜ Overview

### ğŸ“¥ Download Link

The dataset includes:

* **Processed HDF5 files (~3.43 GB)**
* **Raw mmWave + audio data (~87 GB)**
  
The dataset can be downloaded here: https://pan.ustc.edu.cn/share/index/9fb5b7a0cc3247d0bb85.

**EmoDataset** is a paired **mmWaveâ€“audio emotional speech** dataset collected in an **IRB-approved** study. Over one month, **27 stage actors** (11 male, 16 female, aged 18â€“31) were recruited to perform emotional speech while synchronized **mmWave IF signals** and **audio waveforms** were recorded.

The dataset supports research on multimodal emotion sensing, vocal biomechanics, robust speech modeling, and mmWaveâ€“audio alignment.

---

## ğŸ­ Emotion Taxonomy

A six-category emotion set covering a broad valenceâ€“arousal range:

* **happy**
* **calm**
* **angry**
* **tense**
* **sad**
* **bored**

Each participant performed all six emotions.

---

## ğŸ—£ Speech Sentences

Participants read 18 command-style sentences from *ok-google.io* (Google, 2021):

```
1. How old is Taylor
2. What is the definition of back end
3. Do I need an umbrella for tomorrow
4. Who invented the wheel
5. When will AA one two five land
6. Convert six millimeters to meters
7. Make a note: message my mom
8. Show me the appointments for tomorrow
9. Set an alarm in six minutes
10. Where were you born
11. What is the time at home
12. What is the weather like
13. What is the temperature outside
14. What is Apple trending at
15. What is six centimeters in meters
16. What is six plus three
17. Open gmail dot com
18. Decrease brightness
```

Each sentence appears multiple times across all emotions.

---

## ğŸ“¡ Data Modalities

Two data formats are provided:

1. **Processed HDF5 data**
2. **Raw audio + mmWave IF data**

---

## 1. Processed HDF5 Format

Each sample is a group in an HDF5 file with the naming scheme:

```
emotion_sentenceID_subjectID_repeatIndex
```

**Example:**

```
angry_s10_3_2
```

Meaning:

* `angry` â€” emotion
* `s10` â€” sentence ("Where were you born")
* `3` â€” participant ID
* `2` â€” repetition index

### Keys in each group

* **time_mmwave** â€” mmWave vocal vibration sequence
* **time_audio** â€” synchronized audio waveform
* **word_timing_audio** â€” word-level timing annotations used **only** by the audio teacher model during **offline distillation**; they are **not** used in FeelWaveâ€™s online inference and therefore introduce **no latency**

All data are time-aligned.

---

## 2. Raw Data Format

Raw mmWave IF and audio signals are stored in a hierarchical directory structure.

Example path:

```
FeelWave_Data_Raw/mmwave/angry/s1/1/0
```

Meaning:

* `mmwave` â€” modality
* `angry` â€” emotion
* `s1` â€” sentence ID
* `1` â€” participant ID
* `0` â€” repetition index

Raw data maintain their native sampling rates and radar frame formats.

---

## ğŸ“ Summary Table

| Component       | Description                                |
| --------------- | ------------------------------------------ |
| Participants    | 27 stage actors (aged 18â€“31)               |
| Emotions        | happy, calm, angry, tense, sad, bored      |
| Sentences       | 18 phrases from *ok-google.io*             |
| Modalities      | audio + mmWave IF signals                  |
| Formats         | HDF5 (processed), raw (audio & mmWave)     |
| Synchronization | Time-aligned within each sample            |

---

